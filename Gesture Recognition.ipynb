{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hand Gesture Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Content\n",
    "0. [Import Dependencies](#install)\n",
    "1. [Hands Detection using MediaPipe](#detection) \n",
    "2. [Feature Extraction](#feature-extraction) \n",
    "    1. [Write Columns Head in CSV File](#csv-header)\n",
    "    2. [Extract Features of Assigned Class](#save-coordinates)\n",
    "3. [Train Neural Network Using Tensorflow](#model)\n",
    "    1. [Load and Preprocess Input Data](#load-input)\n",
    "    2. [Train A MLP Model](#training)\n",
    "    3. [Evaluate and Serialize Model](#evaluate)\n",
    "4. [Real-time Detections with Model](#real-time-detection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Import Dependencies <a id=\"install\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mediapipe import solutions as mp\n",
    "import cv2\n",
    "import time\n",
    "import os\n",
    "import csv\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keyboard  \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk import pos_tag, word_tokenize, RegexpParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Hands Detection using MediaPipe <a id=\"detection\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To connect your phone camera:\n",
    "- Install `DroidCam` App on your phone and `DroidCam Client` on your laptop. \n",
    "- Put the IP address in the `cv2.VideoCapture()` below.\n",
    "- Or use other phone camera IP service provider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Using phone camera ip with DroidCam\n",
    "# Webcam is 0\n",
    "#cap = cv2.VideoCapture(\"http://192.168.100.6:4747/mjpegfeed?640x480\")\n",
    "# cap = cv2.VideoCapture(\"http://192.168.0.160:4747/video?640x480\")\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "pTime = 0\n",
    "\n",
    "# represents the top left corner of rectangle\n",
    "start_point = (300, 100)\n",
    "  \n",
    "# represents the bottom right corner of rectangle\n",
    "end_point = (600, 400)\n",
    "  \n",
    "# Blue color in BGR\n",
    "color = (255, 0, 0)\n",
    "  \n",
    "# Line thickness of 2 px\n",
    "thickness = 2\n",
    "\n",
    "with mp.hands.Hands(max_num_hands=1, min_detection_confidence=0.5, min_tracking_confidence=0.5) as hands:\n",
    "    while True:\n",
    "        success, frame = cap.read()\n",
    "        \n",
    "        if not success:\n",
    "            break\n",
    "\n",
    "        # Recolor feed\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Make detections\n",
    "        results = hands.process(image)\n",
    "\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmark in results.multi_hand_landmarks:\n",
    "                \n",
    "                # Draw hand landmarks\n",
    "                mp.drawing_utils.draw_landmarks(image, hand_landmark, mp.hands.HAND_CONNECTIONS)\n",
    "                \n",
    "        \n",
    "        # Display framerate\n",
    "        cTime = time.time()\n",
    "        fps = 1/(cTime-pTime+0.01)\n",
    "        pTime = cTime\n",
    "        cv2.putText(image, f\"FPS: {int(fps)}\", (500, 50), cv2.FONT_HERSHEY_PLAIN,\n",
    "                    2, (255, 0, 0), 2)\n",
    "    \n",
    "        # Recolor for rendering\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        # Draw a rectangle with blue line borders of thickness of 2 px\n",
    "        image = cv2.rectangle(image, start_point, end_point, color, thickness)\n",
    "\n",
    "        \n",
    "        cv2.imshow(\"Hand detection\", image)\n",
    "        \n",
    "        # Press \"q\" to exit\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Extraction <a id=\"feature-extraction\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_coords = len(results.multi_hand_landmarks[0].landmark) \n",
    "num_coords\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write Columns Head in CSV File <a id=\"csv-header\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks = ['class']\n",
    "for val in range(1, num_coords+1):\n",
    "    landmarks += ['x{}'.format(val), 'y{}'.format(val),\n",
    "                  'z{}'.format(val), 'v{}'.format(val)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"data\"):\n",
    "    os.mkdir(\"data\")\n",
    "\n",
    "with open(\"data/hand_gesture_coords1.csv\", mode=\"w\", newline=\"\" ) as f:\n",
    "    csv_writer = csv.writer(f, delimiter=\",\", quotechar='\"',quoting=csv.QUOTE_MINIMAL)\n",
    "    #csv_writer.writerow(landmarks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Features of Assigned Class <a id=\"save-coordinates\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key in the class_name and start training :)\n",
    "class_name = \"Bad\"\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "# cap = cv2.VideoCapture(\"http://192.168.0.160:4747/video?640x480\")\n",
    "\n",
    "pTime = 0\n",
    "\n",
    "# represents the top left corner of rectangle\n",
    "start_point = (300, 100)\n",
    "  \n",
    "# represents the bottom right corner of rectangle\n",
    "end_point = (600, 400)\n",
    "  \n",
    "# Blue color in BGR\n",
    "color = (255, 0, 0)\n",
    "  \n",
    "# Line thickness of 2 px\n",
    "thickness = 2\n",
    "  \n",
    "\n",
    "with mp.hands.Hands(max_num_hands=1, min_detection_confidence=0.5, min_tracking_confidence=0.5) as hands:\n",
    "\n",
    "    while True:\n",
    "        success, frame = cap.read()\n",
    "        \n",
    "        if not success:\n",
    "            break\n",
    "\n",
    "        # Recolor feed\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Make detections\n",
    "        results = hands.process(image)\n",
    "\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmark in results.multi_hand_landmarks:\n",
    "                \n",
    "                # Draw hand landmarks\n",
    "                mp.drawing_utils.draw_landmarks(image, hand_landmark, mp.hands.HAND_CONNECTIONS)\n",
    "                \n",
    "                hand = hand_landmark.landmark\n",
    "                row = list(np.array([[landmark.x, landmark.y, landmark.z, landmark.visibility] \n",
    "                                          for landmark in hand]).flatten())\n",
    "                \n",
    "                # Append class name\n",
    "                row.insert(0, class_name)\n",
    "            \n",
    "                # Export to CVS\n",
    "                with open(\"data/3_hand_gesture_coords.csv\", mode=\"a\", newline=\"\" ) as f:\n",
    "                    csv_writer = csv.writer(f, delimiter=\",\", quotechar='\"',quoting=csv.QUOTE_MINIMAL)\n",
    "                    csv_writer.writerow(row)  \n",
    "         \n",
    "                \n",
    "                # Display framerate\n",
    "                cTime = time.time()\n",
    "                fps = 1/(cTime-pTime+0.01)\n",
    "                pTime = cTime\n",
    "                cv2.putText(image, f\"FPS: {int(fps)}\", (500, 50), cv2.FONT_HERSHEY_PLAIN,\n",
    "                            2, (255, 0, 0), 2)\n",
    "    \n",
    "        # Recolor for rendering\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        # Draw a rectangle with blue line borders of thickness of 2 px\n",
    "        image = cv2.rectangle(image, start_point, end_point, color, thickness)\n",
    "\n",
    "\n",
    "        cv2.imshow(\"Hand detection\", image)\n",
    "\n",
    "        # Press \"q\" to exit\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train Nueral Network Using Tensorflow <a id=\"model\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Preprocess Input Data <a id=\"load-input\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/3_hand_gesture_coords.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['class'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into input and output columns\n",
    "X = df.drop(\"class\", axis=1)\n",
    "y = df[\"class\"]\n",
    "\n",
    "# Ensure all data are floating point values\n",
    "X = X.astype('float32')\n",
    "\n",
    "# Encode strings to integer\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "np.save(\"data/hand_geture_classes.npy\", le.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "print('Original dataset shape %s' % Counter(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#smote\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "over = SMOTE(random_state=42, n_jobs=-1, k_neighbors=4)\n",
    "under = RandomUnderSampler(sampling_strategy=0.5)\n",
    "steps = [('o', over), ('u', under)]\n",
    "pipeline = Pipeline(steps=steps)\n",
    "\n",
    "X_res, y_res = over.fit_resample(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Resampled dataset shape %s' % Counter(y_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.2, random_state=47)\n",
    "\n",
    "# Determine the number of input features\n",
    "n_features = X_train.shape[1]\n",
    "\n",
    "# Determine the number or classes\n",
    "n_class = len(np.unique(y_train))\n",
    "\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train A MLP Model <a id=\"training\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a model with the sequential api\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, activation='relu', kernel_initializer='he_normal', input_shape=(n_features,)))\n",
    "    model.add(Dense(10, activation='relu', kernel_initializer='he_normal'))\n",
    "    model.add(Dense(n_class, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "model = create_model()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=32, verbose=0, validation_split=0.2, shuffle=True,\n",
    "                   use_multiprocessing=True)\n",
    "\n",
    "# Plot learning curves\n",
    "plt.title('Learning Curves')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='val')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "# from sklearn.model_selection import StratifiedKFold\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate and Serialize Model <a id=\"evaluate\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "loss, acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test Accuracy: %.3f' % acc)\n",
    "\n",
    "# Make a prediction\n",
    "row = np.random.randn(1, n_features)\n",
    "yhat = model.predict([row])\n",
    "print('Predicted: %s (class=%d | %s)' % (yhat, np.argmax(yhat), list(le.inverse_transform([np.argmax(yhat)]))[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"generated_model\"):\n",
    "    os.mkdir(\"generated_model\")\n",
    "    \n",
    "# Save model to file\n",
    "model.save(\"generated_model/3.1a_hand_gesture_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Real-time Detections with Model <a id=\"real-time-detection\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model from file\n",
    "model_inference = load_model(\"generated_model/3.1a_hand_gesture_model.h5\")\n",
    "\n",
    "# Load the class names\n",
    "le = LabelEncoder()\n",
    "le.classes_ = np.load(\"data/hand_geture_classes.npy\", allow_pickle=True)\n",
    "\n",
    "# Define a text list to store result\n",
    "text = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SignDetection(text):\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    pTime = 0\n",
    "\n",
    "    # represents the top left corner of rectangle\n",
    "    start_point = (300, 60)\n",
    "\n",
    "    # represents the bottom right corner of rectangle\n",
    "    end_point = (600, 380)\n",
    "\n",
    "    # Blue color in BGR\n",
    "    color = (255, 0, 0)\n",
    "\n",
    "    # Line thickness of 2 px\n",
    "    thickness = 2\n",
    "\n",
    "    result = []\n",
    "\n",
    "    with mp.hands.Hands(max_num_hands=1, min_detection_confidence=0.5, min_tracking_confidence=0.5) as hands:\n",
    "\n",
    "        while True:\n",
    "            success, frame = cap.read()\n",
    "\n",
    "            if not success:\n",
    "                break\n",
    "\n",
    "            # Recolor feed\n",
    "            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            # Make detections\n",
    "            results = hands.process(image)\n",
    "\n",
    "            if results.multi_hand_landmarks:\n",
    "                for hand_landmark in results.multi_hand_landmarks:\n",
    "\n",
    "                    # Draw hand landmarks\n",
    "                    mp.drawing_utils.draw_landmarks(image, hand_landmark, mp.hands.HAND_CONNECTIONS)\n",
    "\n",
    "                    hand = hand_landmark.landmark\n",
    "                    row = np.array([[landmark.x, landmark.y, landmark.z, landmark.visibility] \n",
    "                                              for landmark in hand]).flatten()\n",
    "\n",
    "                    # Predict using inference model\n",
    "                    pred = model_inference.predict(row.reshape(1, -1))\n",
    "                    class_name = list(le.inverse_transform([np.argmax(pred)]))[0]\n",
    "                    prob = np.max(pred).round(2)\n",
    "                    output = ''.join(result)  # Define Output Text\n",
    "                    #print(class_name, prob)  \n",
    "\n",
    "                    # Display result \n",
    "                    cv2.rectangle(image, (0,0), (250,60), (245, 117, 16), -1)\n",
    "                    cv2.putText(image, \"CLASS\", (95,12), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                               0.5, (0,0,0), 1, cv2.LINE_AA)\n",
    "                    cv2.putText(image, class_name, (90,40), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                               1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "                    cv2.putText(image, \"PROB\", (15,12), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                               0.5, (0,0,0), 1, cv2.LINE_AA)\n",
    "                    cv2.putText(image, str(prob), (10,40), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                               1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "                    # Display Output Text\n",
    "                    cv2.rectangle(image, (0,400), (650,500), (245, 117, 16), -1)\n",
    "                    cv2.putText(image, \"RESULT\", (15,420), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                               0.5, (0,0,0), 1, cv2.LINE_AA)\n",
    "                    cv2.putText(image, output, (10,460), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                               1, (255, 255, 255), 2, cv2.LINE_AA) \n",
    "\n",
    "\n",
    "                    # Display framerate\n",
    "                    cTime = time.time()\n",
    "                    fps = 1/(cTime-pTime)\n",
    "                    pTime = cTime\n",
    "                    cv2.putText(image, f\"FPS: {int(fps)}\", (500, 50), cv2.FONT_HERSHEY_PLAIN,\n",
    "                                2, (255, 0, 0), 2)\n",
    "\n",
    "                    # Retrieve Output Text\n",
    "                    if (prob >= 0.8) & (keyboard.is_pressed(\"enter\")):  # Detect 'm' input\n",
    "                        result.append(class_name)\n",
    "                        time.sleep(1)\n",
    "\n",
    "                    if (keyboard.is_pressed(\"space\")):  # Detect 'n' input to add space\n",
    "                        result.append(' ')\n",
    "                        time.sleep(1)\n",
    "\n",
    "\n",
    "            # Recolor for rendering\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "            # Draw a rectangle with blue line borders of thickness of 2 px\n",
    "            image = cv2.rectangle(image, start_point, end_point, color, thickness)\n",
    "\n",
    "\n",
    "            cv2.imshow(\"Hand detection\", image)\n",
    "\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    # Store into text list\n",
    "    if result:   #If not empty list, then append\n",
    "        text.append(''.join(result))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/datamunge/sign-language-mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UI Console"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# assign data of lists.  \n",
    "data = {'Name': ['MOCHA', 'LATTE', 'CHOCO', 'APPLE', 'OREN', 'MANGO', 'PANDAN', 'CARAMEL', 'CARROT'], \n",
    "        'Category': ['COFFEE', 'COFFEE', 'COFFEE', 'JUICE', 'JUICE', 'JUICE', 'CAKE', 'CAKE', 'CAKE'],\n",
    "        'Price': [8.99, 9.99, 10.99, 7.99, 6.99, 5.99, 12.59, 10.10, 15.20]}  \n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Menu(text):\n",
    "    rslt_df = df.loc[df['Category'] == text[0]]\n",
    "    rslt_df = rslt_df.reset_index(drop=True)\n",
    "    print(\"\\nPlease Pick One From Below: \")\n",
    "    print(\"Name        Price\")\n",
    "    print(\"-----------------\")\n",
    "    for x in range(len(rslt_df)):\n",
    "        print('%s       %.2f' % (rslt_df['Name'][x], rslt_df['Price'][x]))\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Price(text):\n",
    "    rslt_df = df.loc[df['Name'] == text[1]]\n",
    "    rslt_df = rslt_df.reset_index(drop=True)\n",
    "    for x in range(len(rslt_df)):\n",
    "        text.append(rslt_df['Price'][x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Main(text): \n",
    "    # Define a text list to store result\n",
    "#     text = ['CAKE', 'CARROT']\n",
    "    a = input(\"ZhenLi Coffee Shop \\n\" \n",
    "              \"Please Pick One Category to Proceed \\n\"\n",
    "              \" - Coffee \\n - Juice \\n - Cake \\n\"\n",
    "              \"1. Start Sign Detection \\n\"\n",
    "              \"Your input is: \")\n",
    "    userinput = int(a)\n",
    "    \n",
    "    if (userinput == 1):\n",
    "        print(\"\\nOpening Camera....\")\n",
    "        print(\"Tips: 'Enter' to confirm sign || 'q' to end program\")\n",
    "        SignDetection(text) # Return user input\n",
    "        print(\"\\nYou have picked : \" + text[0])\n",
    "        rslt_df = df.loc[df['Category'] == text[0]]\n",
    "        Menu(text)  #Print all menu item\n",
    "        \n",
    "        b = input(\"1. Start Sign Detection \\n\"\n",
    "                  \"Your input is: \")\n",
    "        userinput = int(b)\n",
    "        if (userinput == 1):\n",
    "            print(\"\\nOpening Camera....\")\n",
    "            print(\"Tips: 'Enter' to confirm sign || 'q' to end program\")\n",
    "            SignDetection(text)\n",
    "            print(\"\\nYou have picked : \" + text[1])\n",
    "        else: \n",
    "            raise Exception(\"Please Enter Correct Input\") \n",
    "        \n",
    "        Price(text)    #Find the price\n",
    "        \n",
    "        print(\"\\n\\n--------------\")\n",
    "        print(\"Your Orders:\")\n",
    "        print(\"--------------\")\n",
    "        print(\"You have ordered a %s %s\" %(text[1],text[0]))\n",
    "        print(\"Total Price: RM %.2f\" %text[2])\n",
    "    else: \n",
    "        raise Exception(\"Please Enter Correct Input\") \n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=[]\n",
    "text = Main(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pos Taging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame\n",
    "df = pd.DataFrame(columns = ['text', 'tagged_text'])\n",
    "\n",
    "for x in text:\n",
    "    print(x)\n",
    "    tag_text = word_tokenize(x)\n",
    "    pos_text = nltk.pos_tag(tag_text)\n",
    "    #Insert into dataframe\n",
    "    df_length = len(df)\n",
    "    df.loc[df_length] = [x,pos_text]\n",
    "    \n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing (Parse Tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all parts of speech from any text\n",
    "chunker = RegexpParser(\"\"\"\n",
    "                       NP: {<DT>?<JJ>*<NN>}    #To extract Noun Phrases\n",
    "                       P: {<IN>}               #To extract Prepositions\n",
    "                       V: {<V.*>}              #To extract Verbs\n",
    "                       PP: {<p> <NP>}          #To extract Prepositional Phrases\n",
    "                       VP: {<V> <NP|PP>*}      #To extract Verb Phrases\n",
    "                       \"\"\")\n",
    "\n",
    "# Print all parts of speech in above sentence\n",
    "for x in range(len(df)):\n",
    "    print(\"Before Extracting\\n\", df['text'][x])\n",
    "    output = chunker.parse(df['tagged_text'][x])\n",
    "    print(\"After Extracting\\n\", output)\n",
    "    \n",
    "    # To draw the parse tree\n",
    "    output.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grammar Designing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for displaying multiple images in one figure\n",
    "  \n",
    "#import libraries\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "  \n",
    "# create figure\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "  \n",
    "# setting values to rows and column variables\n",
    "rows = 2\n",
    "columns = 5\n",
    "  \n",
    "# reading images\n",
    "Image1 = cv2.imread('sign_pic/hello.png')\n",
    "Image2 = cv2.imread('sign_pic/thank you.png')\n",
    "Image3 = cv2.imread('sign_pic/love.png')\n",
    "Image4 = cv2.imread('sign_pic/call me.png')\n",
    "Image5 = cv2.imread('sign_pic/yes.png')\n",
    "Image6 = cv2.imread('sign_pic/no.png')\n",
    "Image7 = cv2.imread('sign_pic/bye.png')\n",
    "Image8 = cv2.imread('sign_pic/ok.png')\n",
    "Image9 = cv2.imread('sign_pic/nice.png')\n",
    "Image10 = cv2.imread('sign_pic/bad.png')\n",
    "\n",
    "# Adds a subplot at the 1st position\n",
    "fig.add_subplot(rows, columns, 1)\n",
    "  \n",
    "# showing image\n",
    "plt.imshow(Image1)\n",
    "plt.axis('off')\n",
    "plt.title(\"Hello\")\n",
    "  \n",
    "# Adds a subplot at the 2nd position\n",
    "fig.add_subplot(rows, columns, 2)\n",
    "  \n",
    "# showing image\n",
    "plt.imshow(Image2)\n",
    "plt.axis('off')\n",
    "plt.title(\"Thank you\")\n",
    "  \n",
    "# Adds a subplot at the 3rd position\n",
    "fig.add_subplot(rows, columns, 3)\n",
    "  \n",
    "# showing image\n",
    "plt.imshow(Image3)\n",
    "plt.axis('off')\n",
    "plt.title(\"Love\")\n",
    "  \n",
    "# Adds a subplot at the 4th position\n",
    "fig.add_subplot(rows, columns, 4)\n",
    "  \n",
    "# showing image\n",
    "plt.imshow(Image4)\n",
    "plt.axis('off')\n",
    "plt.title(\"Call me\")\n",
    "\n",
    "# Adds a subplot at the 4th position\n",
    "fig.add_subplot(rows, columns, 5)\n",
    "  \n",
    "# showing image\n",
    "plt.imshow(Image5)\n",
    "plt.axis('off')\n",
    "plt.title(\"Yes\")\n",
    "\n",
    "# Adds a subplot at the 4th position\n",
    "fig.add_subplot(rows, columns, 6)\n",
    "  \n",
    "# showing image\n",
    "plt.imshow(Image6)\n",
    "plt.axis('off')\n",
    "plt.title(\"No\")\n",
    "\n",
    "# Adds a subplot at the 4th position\n",
    "fig.add_subplot(rows, columns, 7)\n",
    "  \n",
    "# showing image\n",
    "plt.imshow(Image7)\n",
    "plt.axis('off')\n",
    "plt.title(\"Bye\")\n",
    "\n",
    "# Adds a subplot at the 4th position\n",
    "fig.add_subplot(rows, columns, 8)\n",
    "  \n",
    "# showing image\n",
    "plt.imshow(Image8)\n",
    "plt.axis('off')\n",
    "plt.title(\"Ok\")\n",
    "\n",
    "# Adds a subplot at the 4th position\n",
    "fig.add_subplot(rows, columns, 9)\n",
    "  \n",
    "# showing image\n",
    "plt.imshow(Image9)\n",
    "plt.axis('off')\n",
    "plt.title(\"Nice\")\n",
    "\n",
    "# Adds a subplot at the 4th position\n",
    "fig.add_subplot(rows, columns, 10)\n",
    "  \n",
    "# showing image\n",
    "plt.imshow(Image10)\n",
    "plt.axis('off')\n",
    "plt.title(\"Bad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df1 = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = []\n",
    "\n",
    "def Test(text1): \n",
    "    # Define a text list to store result\n",
    "#     text = ['CAKE', 'CARROT']\n",
    "    a = input(\"1. Start Sign Detection \\n\"\n",
    "              \"Your input is: \")\n",
    "    userinput = int(a)\n",
    "    \n",
    "    if (userinput == 1):\n",
    "        print(\"\\nOpening Camera....\")\n",
    "        print(\"Tips: 'Enter' to confirm sign || 'q' to end program\")\n",
    "        SignDetection(text1) # Return user input\n",
    "        print(\"\\nYour Sentence is: \" + ' '.join(map(str, text1)))\n",
    "    \n",
    "    else: \n",
    "        raise Exception(\"Please Enter Correct Input\") \n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = Test(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame\n",
    "df1 = pd.DataFrame(columns = ['text', 'tagged_text'])\n",
    "\n",
    "for x in text1:\n",
    "    print(x)\n",
    "    tag_text = word_tokenize(x)\n",
    "    pos_text = nltk.pos_tag(tag_text)\n",
    "    #Insert into dataframe\n",
    "    df1_length = len(df1)\n",
    "    df1.loc[df1_length] = [x,pos_text]\n",
    "    \n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all parts of speech from any text\n",
    "chunker = RegexpParser(\"\"\"\n",
    "                       NP: {<DT>?<JJ>*<NN>}    #To extract Noun Phrases\n",
    "                       P: {<IN>}               #To extract Prepositions\n",
    "                       V: {<V.*>}              #To extract Verbs\n",
    "                       PP: {<p> <NP>}          #To extract Prepositional Phrases\n",
    "                       VP: {<V> <NP|PP>*}      #To extract Verb Phrases\n",
    "                       \"\"\")\n",
    "\n",
    "# Print all parts of speech in above sentence\n",
    "for x in range(len(df1)):\n",
    "    print(\"Before Extracting\\n\", df1['text'][x])\n",
    "    output = chunker.parse(df1['tagged_text'][x])\n",
    "    print(\"After Extracting\\n\", output)\n",
    "    \n",
    "    # To draw the parse tree\n",
    "    output.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
